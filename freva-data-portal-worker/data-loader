#!/bin/sh
set -e

# Set MAMBA_DIR based on write permissions to /opt
if [ -w /opt ]; then
    MAMBA_DIR=/opt/mamba
else
    MAMBA_DIR=~/.mamba
fi

# Determine if this is a worker based on the argument
if [ "$1" = "worker" ]; then
    IS_WORKER="worker"
fi

# Export the environment variable
export IS_WORKER

# Define the packages to be installed
PKGS="\
asyncssh \
bokeh \
cfgrib \
cryptography \
cloudpickle \
dask[distributed,diagnostics] \
h5netcdf \
jupyter-server-proxy \
libgdal \
netcdf4 \
rasterio \
redis-py \
rioxarray \
xarray \
xpublish \
zarr \
mamba \
git \
jq \
zarr"


# Generate a checksum of the package list
MD5SUM=$(echo $PKGS | md5sum | awk '{print $1}')

# Check if the environment needs to be created or updated
if [ ! -f ${MAMBA_DIR}/environment.txt ]; then
    create=true
elif [ "$(cat ${MAMBA_DIR}/environment.txt)" != "$MD5SUM" ]; then
    create=true
else
    create=false
fi

# Create a lock file name based on MAMBA_DIR
lock_file=$(echo $MAMBA_DIR | sed 's#/#-#g')
lock_file="/tmp/conda-${lock_file}.lock"
max_age=$((60 * 60)) # 1 hour in seconds

# Function to remove the lock file
cleanup() {
    rm -f "$lock_file"
}
trap cleanup EXIT

# Wait until the lock file is removed or is older than 1 hour
while [ -e "$lock_file" ]; do
    file_mod_time=$(stat -c %Y "$lock_file")
    current_time=$(date +%s)
    file_age=$((current_time - file_mod_time))
    if [ $file_age -gt $max_age ]; then
        break
    fi
    sleep 10
done

# Create the lock file
touch "$lock_file"

echo "MAMBA_DIR=$MAMBA_DIR" > ~/.config/dask-worker-env

if [ "$create" = true ]; then
    echo "Setting up miniconda:"
    rm -rf ${MAMBA_DIR}
    ARCH=$(uname -m)
    CWD=$(readlink -f $(dirname $0))
    TEMP_DIR=$(mktemp -d)
    cd "$TEMP_DIR" || { echo "Failed to create temporary directory"; exit 1; }

    case "$ARCH" in
        x86_64)
            curl -s https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh
            ;;
        aarch64)
            curl -s https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -o conda.sh
            ;;
        ppc64le)
            curl -s https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-ppc64le.sh -o conda.sh
            ;;
        armv7l)
            curl -s https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-armv7l.sh -o conda.sh
            ;;
        *)
            echo "Unsupported architecture: $ARCH"
            exit 1
            ;;
    esac

    /bin/sh conda.sh -b -u -f -p ${MAMBA_DIR}
    ${MAMBA_DIR}/bin/conda install -c conda-forge -y $PKGS
    echo $MD5SUM > ${MAMBA_DIR}/environment.txt
    rm -fr $TEMP_DIR
fi

if [ ! -f ${MAMBA_DIR}/bin/data-loader-worker ]; then
    echo "Setting up the dataportal:"
    TEMP_DIR=$(mktemp -d)
    cd "$TEMP_DIR" || { echo "Failed to create temporary directory"; exit 1; }
    ${MAMBA_DIR}/bin/git clone --recursive -b stream-data https://github.com/FREVA-CLINT/freva-nextgen.git
    rm -rf ${MAMBA_DIR}/freva-data-portal-worker
    cp -r freva-nextgen/freva-data-portal-worker ${MAMBA_DIR}/
    cp -r freva-nextgen/dev-env ${MAMBA_DIR}/freva-data-portal-worker/setup
    cp freva-nextgen/run_server.py ${MAMBA_DIR}/freva-data-portal-worker/
    cd ${MAMBA_DIR}/freva-data-portal-worker/
    ${MAMBA_DIR}/bin/python -m pip install .
    rm -fr $TEMP_DIR
fi

if ([ ! -f "$HOME/.data-portal-cluster-config.json" ] && [ "x$IS_WORKER" = x ]); then
    TEMP_DIR=~ ${MAMBA_DIR}/bin/python ${MAMBA_DIR}/freva-data-portal-worker/run_server.py --gen-certs --script-dir ${MAMBA_DIR}/freva-data-portal-worker/setup
fi

export PATH=${MAMBA_DIR}/bin:$PATH
redis_host=$(base64 -d ~/.data-portal-cluster-config.json | jq -r .host)
dask_host=$(base64 -d ~/.data-portal-cluster-config.json | jq -r .scheduler_host)
scheduler_port=${dask_host##*:}
scheduler_port=${scheduler_port%%/*}
port_part=${redis_host##*:}
redis_port=${port_part%%/*}

if [ "x$IS_WORKER" = x ]; then
    user=$(base64 -d ~/.data-portal-cluster-config.json | jq -r .user)
    passwd=$(base64 -d ~/.data-portal-cluster-config.json | jq -r .passwd)
    cache_exp=$(base64 -d ~/.data-portal-cluster-config.json | jq -r .cache_exp)
    CMD="${MAMBA_DIR}/bin/python -m data_portal_worker \
        -c $HOME/.data-portal-cluster-config.json -e \
        $cache_exp -r redis://localhost:$redis_port -p \
        $scheduler_port"
else
    CMD="${MAMBA_DIR}/bin/python -m distributed.cli.dask_worker \
        ${dask_host} --nanny --nworkers -1 --nthreads 2"
fi

echo $CMD
$CMD

